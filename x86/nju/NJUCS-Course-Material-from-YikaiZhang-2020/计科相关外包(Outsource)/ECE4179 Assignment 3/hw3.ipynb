{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "the following class reads the data for the third assignment and creates a torch dataset object for it. With this, you can easily use a dataloader to train your model. \n",
    "\n",
    "Due to size limit on moodle, the data for this assignment should be obtained from \n",
    "\n",
    "https://drive.google.com/file/d/1Nj8HK180dVj-Y9b2w2hRGz726c8OTF_C/view?usp=sharing\n",
    "\n",
    "\n",
    "Make sure that the file \"hw3.npz\" is located properly (in this example, it should be in the same folder as this notebook.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STLData(Dataset):\n",
    "    def __init__(self,trn_val_tst = 0, transform=None):\n",
    "        data = np.load('hw3.npz')\n",
    "        if trn_val_tst == 0:\n",
    "            #trainloader\n",
    "            self.images = data['arr_0']\n",
    "            self.labels = data['arr_1']\n",
    "        elif trn_val_tst == 1:\n",
    "            #valloader\n",
    "            self.images = data['arr_2']\n",
    "            self.labels = data['arr_3']\n",
    "        else:\n",
    "            #testloader\n",
    "            self.images = data['arr_4']\n",
    "            self.labels = data['arr_5']\n",
    "            \n",
    "        self.images = np.float32(self.images)/1.0\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = self.images[idx,:]\n",
    "        labels = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how you can create a dataloader. \n",
    "First read the data. Note that the STL10 class can work with torchvision.transforms that are required in HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据, 并设置相应的预处理变换:\n",
    "train_set = STLData(trn_val_tst=0, transform=torchvision.transforms.ToTensor())\n",
    "val_set = STLData(trn_val_tst=1, transform=torchvision.transforms.ToTensor())\n",
    "test_set = STLData(trn_val_tst=2, transform=torchvision.transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a batchsize of 100, you can have a dataloader as follows for your training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照相应batch size导入数据:\n",
    "batch_size = 100 \n",
    "n_workers = multiprocessing.cpu_count()\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据示例:\n",
    "image_batch, labels = next(iter(trainloader))\n",
    "for tmpC1 in range(8):\n",
    "    img = np.moveaxis(image_batch[tmpC1].numpy(),0,2)\n",
    "    plt.subplot(2,4,tmpC1+1)\n",
    "    plt.imshow(img/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, 7, 2)\n",
    "        self.conv2 = nn.Conv2d(96, 64, 5, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 2)\n",
    "        self.fc1 = nn.Linear(1152, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool1 = nn.MaxPool2d(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 1152)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "import tqdm\n",
    "\n",
    "loss_record, test_loss_record, val_loss_record = [], [], []\n",
    "test_correct_record, val_correct_record, correct_record = [], [], []\n",
    "correct, total = 0, 0\n",
    "for epoch in tqdm.trange(8):\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        labels = labels.long()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += pred.shape[0]\n",
    "        loss_record.append(loss.item())\n",
    "        correct_record.append(correct / total)\n",
    "\n",
    "\n",
    "    # print(correct / total)\n",
    "    test_loss, val_loss = 0, 0\n",
    "    test_correct, val_correct = 0, 0\n",
    "    test_total, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            labels = labels.long()\n",
    "\n",
    "            net.eval()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            test_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            test_total += pred.shape[0]\n",
    "        # print(test_correct / test_total)\n",
    "        test_loss_record.append(test_loss / len(testloader.dataset))\n",
    "        test_correct_record.append(test_correct / test_total)\n",
    "\n",
    "        for i, data in enumerate(valloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            val_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        val_loss_record.append(val_loss / len(valloader.dataset))\n",
    "        val_correct_record.append(val_correct / len(valloader.dataset))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "def my_plt(fig, name):\n",
    "    plt.plot(fig)\n",
    "    plt.savefig(name + '.png')\n",
    "    plt.close()\n",
    "\n",
    "my_plt(loss_record, 'loss_record_1')\n",
    "\n",
    "my_plt(val_loss_record, 'val_loss_record_1')\n",
    "\n",
    "my_plt(correct_record, 'correct_record_1')\n",
    "\n",
    "my_plt(val_correct_record, 'val_correct_record_1')\n",
    "\n",
    "my_plt(test_loss_record, 'test_loss_record_1')\n",
    "\n",
    "my_plt(test_correct_record, 'test_correct_record_1')\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "class AdvancedNet(nn.Module):\n",
    "    def __init__(self, block, num_classes=10):\n",
    "        super(AdvancedNet, self).__init__()\n",
    "        self.layer1 = self.make_layer(block, 3, 32)\n",
    "        self.layer2 = self.make_layer(block, 32, 64)\n",
    "        self.layer3 = self.make_layer(block, 64, 128)\n",
    "        self.layer4 = self.make_layer(block, 128, 192)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(192, num_classes)\n",
    "\n",
    "    def make_layer(self, block, in_channels, out_channels):\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "net = AdvancedNet(ConvBlock)\n",
    "net.to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "loss_record, test_loss_record, val_loss_record = [], [], []\n",
    "test_correct_record, val_correct_record, correct_record = [], [], []\n",
    "correct, total = 0, 0\n",
    "for epoch in tqdm.trange(8):\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        labels = labels.long()\n",
    "        loss = criterion(outputs, labels)\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += pred.shape[0]\n",
    "        loss_record.append(loss.item())\n",
    "        correct_record.append(correct / total)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print(correct / total)\n",
    "    net.eval()\n",
    "    test_loss, val_loss = 0, 0\n",
    "    test_correct, val_correct = 0, 0\n",
    "    test_total, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            test_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            test_total += pred.shape[0]\n",
    "        # print(test_correct / test_total)\n",
    "        test_loss_record.append(test_loss / len(testloader.dataset))\n",
    "        test_correct_record.append(test_correct / test_total)\n",
    "\n",
    "        for i, data in enumerate(valloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            val_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        val_loss_record.append(val_loss / len(valloader.dataset))\n",
    "        val_correct_record.append(val_correct / len(valloader.dataset))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "def my_plt(fig, name):\n",
    "    plt.plot(fig)\n",
    "    plt.savefig(name + '.png')\n",
    "    plt.close()\n",
    "\n",
    "my_plt(loss_record, 'loss_record_2')\n",
    "\n",
    "my_plt(val_loss_record, 'val_loss_record_2')\n",
    "\n",
    "my_plt(correct_record, 'correct_record_2')\n",
    "\n",
    "my_plt(val_correct_record, 'val_correct_record_2')\n",
    "\n",
    "my_plt(test_loss_record, 'test_loss_record_2')\n",
    "\n",
    "my_plt(test_correct_record, 'test_correct_record_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
